# Heavy Analysis Module

## Overview

This module contains computationally intensive data science operations for discovering patterns and equipment types within Cygnet facility data. The goal is to create a **generic, company-agnostic system** that can analyze any Cygnet installation and discover its structure without hardcoded assumptions.

---

## Phase 1: Attribute Analysis (`attribute_analysis.py`)

### Purpose
Discover natural facility groupings by analyzing which attributes are filled and how they vary across facilities.

### Strategy

#### 1. **Attribute Profiling**
- Leverages pre-computed statistics from `summaries.json` (already generated by `parse-fac`)
- Calculates **discriminative power** for each column:
  - Empty columns = 0 score (not useful)
  - Columns with 1 unique value = 0 score (not discriminative)
  - Columns with moderate fill rates (20-80%) + high uniqueness = highest scores
  - Formula: `(fill_score * 0.4) + (uniqueness_score * 0.6)`

#### 2. **Column Categorization**
Automatically categorizes columns:
- **Structural**: `site`, `service`, `is_active` (always present, low variance)
- **Identity**: `id` (unique per facility)
- **Descriptive**: `desc`, `type`, `category`, `info0`, `info1` (human-readable labels)
- **Attributes**: `attr0-39` (custom company-specific data)
- **Tables**: `table0-59` (reference tables)
- **Flags**: `yes_no0-19` (boolean settings)

#### 3. **Attribute Signature Discovery**
- Creates "signatures" = patterns of which columns are filled
- Example signature: `{type: filled, category: filled, attr2: filled, attr7: empty, ...}`
- Facilities with similar signatures likely represent similar equipment types
- Groups facilities by their signatures to find natural clusters

#### 4. **Value Pattern Analysis**
- For the most discriminative columns, analyzes common values
- Example: `type` column might have values like "WELL", "METER", "SEPARATOR"
- Identifies the most frequent values and their distributions

### Output

The analysis produces:
```json
{
  "total_facilities": 5620,
  "key_attributes_count": 15,
  "top_5_attributes": [
    {"column": "type", "score": 0.875, "fill_rate": 0.962},
    {"column": "category", "score": 0.742, "fill_rate": 1.0},
    ...
  ],
  "facility_groups_found": 8,
  "group_coverage_pct": 94.5,
  "recommendations": [
    "✓ Top discriminative attributes: type, category, info1",
    "✓ Discovered 8 natural facility groups covering 94.5% of facilities",
    "→ Next: Analyze descriptions for semantic patterns"
  ]
}
```

---

## Future Phases (Roadmap)

### Phase 2: Description Analysis (Planned)
- Extract keywords from `desc`, `info0`, `info1` fields
- Build vocabulary of equipment terms (pump, meter, tank, well, separator)
- Use NLP techniques (TF-IDF, word embeddings) to find semantic patterns
- Correlate keywords with attribute signatures

### Phase 3: UDC Bridging (Planned)
- For each discovered facility group, query PNT dataset
- Find all PNT records with matching `facilityid`
- Extract distinct UDCs used by each facility type
- Build mapping: `Equipment Type → Facility IDs → UDCs → Tag Names`

### Phase 4: Tag Name Pattern Recognition (Planned)
- Analyze tag naming conventions within each equipment group
- Discover patterns like prefixes, suffixes, separators
- Group tags by UDC and equipment type
- Generate tag templates for each equipment category

---

## Usage

### Command Line
```bash
>>> analyze-attributes
```

This will:
1. Load FAC summary from `summaries.json`
2. Read FAC CSV file
3. Run attribute profiling analysis
4. Display results in the console
5. Save analysis results to `summaries.json` under `analysis_attributes`

### Programmatic
```python
from cyg_to_ign.Scripts.heavy_analysis.attribute_analysis import run_attribute_analysis

results = run_attribute_analysis(
    fac_filepath="path/to/fac.csv",
    fac_summary=fac_summary_dict,
    output_detail="detailed"  # "summary", "detailed", or "full"
)
```

---

## Design Principles

1. **Generic & Company-Agnostic**
   - No hardcoded equipment types
   - No assumptions about column usage
   - Discovers patterns from the data itself

2. **Conservative Approach**
   - Starts with high-confidence patterns
   - Allows human refinement at key decision points
   - Transparent scoring and recommendations

3. **Modular & Testable**
   - Heavy analysis separated into dedicated module
   - Each phase is independent
   - Easy to add new analysis methods

4. **Leverages Existing Work**
   - Uses pre-computed summaries from `parse-fac`
   - Integrates with existing validation utilities
   - Extends the merged validation dataset when needed

---

## Key Insights from Initial Planning

### Why Attribute Fill Patterns Matter
- Companies customize Cygnet extensively
- `attr0-39` are generic containers that each company uses differently
- A well facility might always have `attr2`, `attr7`, `attr27` filled
- A meter might have `attr3`, `attr12`, `attr25` filled
- These patterns are **signatures** that identify equipment types

### Why Not Just Use `type` or `category`?
- Not always filled (can be 0-20% empty)
- Not always accurate or up-to-date
- Companies may misuse these fields
- Sometimes custom attributes are more reliable

### The Power of Discovery
- By finding patterns WITHOUT labels, we can:
  - Validate existing `type`/`category` fields
  - Discover unlabeled equipment
  - Find inconsistencies in the data
  - Generate new equipment classifications

---

## Performance Considerations

- **Column Profiling**: Fast (uses pre-computed summaries)
- **Signature Discovery**: Medium (requires full FAC scan, ~5-10 seconds for 5k facilities)
- **Value Pattern Analysis**: Medium (analyzes top 10 columns)
- **Full Analysis**: ~10-20 seconds for typical dataset

For very large datasets (100k+ facilities), consider:
- Sampling strategies
- Incremental analysis
- Caching intermediate results

---

## Testing & Validation

To validate the analysis:
1. Run `analyze-attributes`
2. Check if discovered groups align with known equipment types
3. Cross-reference with `type` and `category` fields
4. Query PNT for sample facilities from each group to see their UDCs
5. Refine discriminative score formula if needed

---

## Next Steps

After running initial attribute analysis:
1. **Review top discriminative attributes** - Do they make sense?
2. **Examine facility groups** - Are there clear natural clusters?
3. **Check value patterns** - Are there meaningful categories?
4. **Plan Phase 2** - Which descriptions should we analyze?
5. **Prepare for UDC bridging** - How should we structure the mapping?
